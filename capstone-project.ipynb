{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Immigration and Tourism Data Analytics\n",
    "### Udacity Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project aims to gather data from 3 different datasources related to US immigration and tourism data, and transform them into a star schema with tables designed to optimize queries on the data. The main goal is to provide trends and insights about the volume of trips and the time of the year.\n",
    "      \n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "* Extra: Write a few analytical queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, StructType, StructField, IntegerType, StringType, FloatType\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "*Explain what you plan to do in the project in more detail:*\n",
    " - Ingest 3 different datasets related to US Immigration Data, Airlines and Global Temperature\n",
    " - Explore and asses the data using this Notebook\n",
    " - Run data quality checks and exclude data that won't be used\n",
    " - Model a star schema to store the final data model \n",
    " - Create an ETL to read the raw data, transform it and load it into a trusted zone area in a datlake.\n",
    " - The analytical tables in the trusted zone will be stored as parquet files\n",
    " - Everything will be ready to be loaded into a DW, if necessary\n",
    " - Provide a few analytical queries to validate the final tables\n",
    " - **The main purpose of the final data model is to esily provide insights about tourists and immigrants behaviours, if it's possible to correlate travels with the time of the year or with certain regions.**\n",
    "\n",
    "*What data do you use?*\n",
    " - I94 Immigration data of 2016\n",
    " - World Temperature Data\n",
    " - World Airports Data\n",
    "\n",
    "*What is your end solution look like?*\n",
    " - 8 tables stored in a star schema designed to optimize queries on immigration data in US\n",
    " - Some analytical queries to validate a few behaviours\n",
    "\n",
    "*What tools did you use?*\n",
    " - Python 3.6\n",
    " - PySpark 2.4.3 using Scala version 2.11.12\n",
    " - Java HotSpot(TM) 64-Bit Server VM, 1.8.0_291\n",
    " - packages in requirements.txt\n",
    " \n",
    "\n",
    "### Describe and Gather Data \n",
    "*Describe the data sets you're using. Where did it come from? What type of information is included?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I94 Immigration Data\n",
    "This data comes from the US National Tourism and Trade Office. Basically, this dataset contains international visitor arrival statistics of the year 2016. \n",
    "\n",
    "The original dataset was in sas7bdat format, but for this project, it was transformed into parquet files since it's a smaller size and also for performance purposes when reading it into Spark dataframes.\n",
    "\n",
    "You can read more about it [here](https://www.trade.gov/national-travel-and-tourism-office)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immig_df = spark.read.parquet(\"./data/raw/i94-sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### World Temperature Data\n",
    "This dataset came from Kaggle. \n",
    "\n",
    "The original dataset was in CSV format, but for this project, it was transformed into parquet files since it's a smaller size and also for performance purposes when reading it into Spark dataframes.\n",
    "\n",
    "You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = spark.read.parquet(\"./data/raw/global_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Port of Entry Codes\n",
    "This dataset contains the three-letter codes that are used by Customs and Border Protection (CBP) in its internal communications to represent ports-of-entry (POEs). It is used in the i94 immigration data in the i94port field. This data came maily from [here](https://fam.state.gov/fam/09FAM/09FAM010205.html), but also some data was merged from the file I94_SAS_Labels_Descriptions.SAS, provided by Udacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports_df = spark.read.options(header='True', inferSchema='True', delimiter=';')\\\n",
    "    .csv(\"./data/raw/port-of-entry-codes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country Codes Table\n",
    "This is a simple dictionary with the codes used in I94 Forms and the corresponding country. This information was obtained from the file I94_SAS_Labels_Descriptions.SAS, provided by Udacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = spark.read.options(header='True', inferSchema='True', delimiter=';')\\\n",
    "    .csv(\"./data/raw/country_codes.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airline Database\n",
    "This dataset came from Kaggle. It contains 5888 airline companies.\n",
    "\n",
    "You can read more about it [here](https://www.kaggle.com/open-flights/airline-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_df = spark.read.options(header='True', inferSchema='True', delimiter=',')\\\n",
    "    .csv(\"./data/raw/airlines.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visa Types\n",
    "This dataset was assembled in a csv file with data from the Department of State - Bureau of Consular Affairs.\n",
    "\n",
    "You can read more about it [here](https://travel.state.gov/content/travel/en/us-visas/visa-information-resources/all-visa-categories.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa_df = spark.read.options(header='True', inferSchema='True', delimiter=',')\\\n",
    "    .csv(\"./data/raw/visa-types.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "*Identify data quality issues, like missing values, duplicate data, etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing the data using SQL:\n",
    "## attempting to join immigration data with other datasets\n",
    "immig_df.createOrReplaceTempView(\"immig_data\")\n",
    "countries_df.createOrReplaceTempView(\"country_codes\")\n",
    "airlines_df.createOrReplaceTempView(\"airlines\")\n",
    "ports_df.createOrReplaceTempView(\"ports\")\n",
    "\n",
    "epoch = dt.datetime(1960, 1, 1).date()\n",
    "spark.udf.register(\"isoformat\", lambda x: (epoch + dt.timedelta(x)).isoformat() if x else None)\n",
    "spark.sql('''\n",
    "          SELECT int(i.cicid) id,\n",
    "                 i.i94port port,\n",
    "                 split(p.location, ',')[0] port_city,\n",
    "                 split(p.location, ',')[1] port_state,\n",
    "                 isoformat(int(i.arrdate)) arrival_date,\n",
    "                 isoformat(int(i.depdate)) departure_date,\n",
    "                 int(i.depdate - i.arrdate) days,\n",
    "                 CASE\n",
    "                     WHEN INT(i.i94mode) = 1 THEN 'Air'\n",
    "                     WHEN INT(i.i94mode) = 2 THEN 'Sea'\n",
    "                     WHEN INT(i.i94mode) = 3 THEN 'Land'\n",
    "                     ELSE 'Not reported'\n",
    "                 END AS mode,\n",
    "                 CASE\n",
    "                     WHEN INT(i.i94visa) = 1 THEN 'Business'\n",
    "                     WHEN INT(i.i94visa) = 2 THEN 'Pleasure'\n",
    "                     ELSE 'Student'\n",
    "                 END AS visa,\n",
    "                 i.visatype,\n",
    "                 int(i.i94bir) age,\n",
    "                 i.gender,\n",
    "                 i.airline,\n",
    "                 a.name airline_name,\n",
    "                 i.fltno flight_no,\n",
    "                 c.country,\n",
    "                 i.occup\n",
    "            FROM immig_data i, \n",
    "                 country_codes c,\n",
    "                 airlines a,\n",
    "                 ports p\n",
    "           WHERE i.gender IS NOT NULL\n",
    "             AND int(i.i94res) = c.code\n",
    "             AND c.country NOT LIKE '%INVALID%'\n",
    "             AND c.country NOT LIKE '%No Country Code%'\n",
    "             and a.IATA = i.airline\n",
    "             and p.code = i.i94port\n",
    "           LIMIT 50\n",
    "    ''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assesssing visa data from i94 data\n",
    "spark.sql('''\n",
    "    SELECT i94visa, count(cicid)\n",
    "      FROM immig_data\n",
    "     GROUP by i94visa\n",
    "     ORDER BY count(cicid) desc\n",
    "     LIMIT 50\n",
    "    ''').toPandas()\n",
    "\n",
    "spark.sql('''\n",
    "    SELECT visatype, count(cicid)\n",
    "      FROM immig_data\n",
    "     GROUP by visatype\n",
    "     ORDER BY visatype desc\n",
    "     LIMIT 20\n",
    "    ''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing port-of-entry data\n",
    "\n",
    "# check how many differents ports we have in the i94 data\n",
    "immig_df.createOrReplaceTempView(\"immig_data\")\n",
    "ports_count = spark.sql('''\n",
    "    SELECT DISTINCT(I94PORT)\n",
    "      FROM immig_data\n",
    "    ''').count()\n",
    "print(f'ports in i94 data: {ports_count}') #397\n",
    "\n",
    "# check if all possible ports are in the ports dataframe\n",
    "ports_df.createOrReplaceTempView(\"ports\")\n",
    "\n",
    "ports_count = spark.sql('''\n",
    "    SELECT code\n",
    "      FROM ports\n",
    "     WHERE code IN (SELECT DISTINCT(I94PORT)\n",
    "                      FROM immig_data)\n",
    "    ''').count()\n",
    "print(f'matches with ports in CBP data: {ports_count}') #396\n",
    "\n",
    "#missing port\n",
    "spark.sql('''\n",
    "    SELECT *\n",
    "      FROM immig_data\n",
    "     WHERE I94PORT NOT IN (SELECT code\n",
    "                           FROM ports)\n",
    "    ''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing temperature dataset\n",
    "# selecting cities in US only\n",
    "# grouping by month to get the average temperature\n",
    "temp_df.createOrReplaceTempView(\"temp\")\n",
    "spark.sql('''\n",
    "    SELECT city, country, month(dt) month, avg(averagetemperature) avg_temp\n",
    "      FROM temp\n",
    "     WHERE lower(country) in ('united states')\n",
    "       AND averagetemperature is not null\n",
    "     GROUP BY city, country, month(dt)\n",
    "     ORDER BY city, country, month(dt)\n",
    "    ''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 100 airports with the highest immigration records\n",
    "immig_df.createOrReplaceTempView(\"immig_data\")\n",
    "top_ports = spark.sql('''\n",
    "\n",
    "    SELECT I94PORT, count(cicid) cnt\n",
    "      FROM immig_data\n",
    "     GROUP BY I94PORT\n",
    "     ORDER BY COUNT(cicid) desc\n",
    "    ''')\n",
    "top_ports.createOrReplaceTempView(\"top_ports\")\n",
    "\n",
    "#cleaning ports outside us or invalid codes\n",
    "clean_ports = spark.sql('''\n",
    "    SELECT code, \n",
    "           split(location, ',')[0] city, \n",
    "           trim(split(location, ',')[1]) state\n",
    "      FROM ports\n",
    "    ''')\n",
    "clean_ports.createOrReplaceTempView(\"clean_ports\")\n",
    "\n",
    "clean_ports = spark.sql('''\n",
    "    SELECT *\n",
    "      FROM clean_ports cp, top_ports tp\n",
    "     WHERE cp.code = tp.i94port\n",
    "       AND cp.state != ''\n",
    "       and length(cp.state) <= 3\n",
    "     ORDER by cnt desc\n",
    "     LIMIT 100\n",
    "    ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ports.createOrReplaceTempView(\"clean_ports\")\n",
    "# port cities with a match in the temperature dataset\n",
    "spark.sql('''\n",
    "    SELECT *\n",
    "      FROM clean_ports p\n",
    "     WHERE p.city in (SELECT DISTINCT(UPPER(city)) \n",
    "                        FROM temp \n",
    "                       WHERE lower(country) in ('united states'))\n",
    " order by p.cnt desc\n",
    "\n",
    "    ''').toPandas()\n",
    "\n",
    "# port cities without a match in the temperature dataset\n",
    "spark.sql('''\n",
    "    SELECT *\n",
    "      FROM clean_ports p\n",
    "     WHERE p.city not in (SELECT DISTINCT(UPPER(city)) \n",
    "                            FROM temp \n",
    "                           WHERE lower(country) in ('united states'))\n",
    " order by p.cnt desc\n",
    "\n",
    "    ''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "Steps necessary to clean the data for each dataset are in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#immigration data\n",
    "\n",
    "#joining with countries_df and removing invalid rows\n",
    "immig_df.createOrReplaceTempView(\"immig_data\")\n",
    "countries_df.createOrReplaceTempView(\"country_codes\")\n",
    "\n",
    "rows_before = immig_df.count()\n",
    "print(f'rows count: {rows_before}')\n",
    "\n",
    "immig_df = spark.sql('''\n",
    "    SELECT i.*, c.country\n",
    "      FROM immig_data i,\n",
    "           country_codes c\n",
    "     WHERE int(i.i94res) = c.code\n",
    "       AND c.country NOT LIKE '%INVALID%'\n",
    "       AND c.country NOT LIKE '%No Country Code%'\n",
    "    ''')\n",
    "\n",
    "rows_after = immig_df.count()\n",
    "print(f'{rows_after - rows_before} invalid rows removed')\n",
    "\n",
    "#dropping columns that won't be used in this project\n",
    "cols = ['i94yr', 'i94mon', 'i94cit', 'i94addr', \n",
    "        'count', 'dtadfile', 'visapost', 'entdepa', \n",
    "        'entdepd', 'entdepu', 'biryear', 'dtaddto', \n",
    "        'i94res', 'matflag']\n",
    "immig_df = immig_df.drop(*cols)\n",
    "\n",
    "#removing duplicates\n",
    "print('removing duplicates')\n",
    "immig_df.dropDuplicates()\n",
    "print(f'rows count after de-duplicate: {immig_df.count()}')\n",
    "\n",
    "#casting data types, relabeling column names and replacing values\n",
    "immig_df.createOrReplaceTempView(\"immig_data\")\n",
    "epoch = dt.datetime(1960, 1, 1).date()\n",
    "spark.udf.register(\"isoformat\", lambda x: (epoch + dt.timedelta(x)).isoformat() if x else None)\n",
    "immig_df = spark.sql('''\n",
    "    SELECT int(i.cicid) id,\n",
    "           i.i94port airport,\n",
    "           isoformat(int(i.arrdate)) arrival_date,\n",
    "           isoformat(int(i.depdate)) departure_date,   \n",
    "           i.i94mode mode,\n",
    "           i.i94visa visa,\n",
    "           i.visatype,\n",
    "           int(i.i94bir) age,\n",
    "           i.gender,\n",
    "           i.airline,\n",
    "           i.fltno flight_num,\n",
    "           i.occup occupation,\n",
    "           i.admnum admission_num,\n",
    "           i.country origin_country\n",
    "      FROM immig_data i\n",
    "    ''')\n",
    "\n",
    "print(f'rows count: {immig_df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ports data\n",
    "\n",
    "# split the location field into city and state\n",
    "# select only the ports that has a match in i94 data\n",
    "\n",
    "ports_df.createOrReplaceTempView(\"ports\")\n",
    "ports_df = spark.sql('''\n",
    "    SELECT code, \n",
    "           split(location, ',')[0] city,\n",
    "           split(location, ',')[1] state\n",
    "      FROM ports a\n",
    "     WHERE code IN (SELECT DISTINCT(I94PORT)\n",
    "                      FROM immig_data)\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature data\n",
    "\n",
    "# selecting cities in US only\n",
    "# grouping by month to get the average temperature\n",
    "temp_df.createOrReplaceTempView(\"temp\")\n",
    "temp_df = spark.sql('''\n",
    "    SELECT city, month(dt) month, avg(averagetemperature) avg_temp\n",
    "      FROM temp\n",
    "     WHERE lower(country) in ('united states')\n",
    "       AND averagetemperature is not null\n",
    "     GROUP BY city, country, month(dt)\n",
    "     ORDER BY city, country, month(dt)\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airlines data\n",
    "\n",
    "# filter out invalid rows\n",
    "# rename columns\n",
    "# select only the ones in i94 immigration data\n",
    "\n",
    "airlines_df.createOrReplaceTempView('airlines')\n",
    "airlines_df = spark.sql('''\n",
    "    SELECT a.name,\n",
    "           a.iata code,\n",
    "           a.country\n",
    "      FROM airlines a\n",
    "     WHERE a.iata is not null\n",
    "       AND a.iata != '-'\n",
    "       AND a.iata IN (SELECT DISTINCT(airline)\n",
    "                      FROM immig_data)\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "*Map out the conceptual data model and explain why you chose that model*\n",
    "\n",
    "The data model consists in a star schema with 1 fact table and 7 dimension tables:\n",
    "\n",
    "##### Fact\n",
    "1. **arrivals** - transformed data from i94 2016 immigration data\n",
    " - id, airport, arrival_date, departure_date, mode, visa, visatype, age, gender, airline, flight_num, occupation, admission_num, origin_country\n",
    "\n",
    "##### Dimensions \n",
    "2. **visa** - visa types details\n",
    "  - type, purpose, code, description\n",
    "\n",
    "\n",
    "3. **i94mode** - arrival mode\n",
    "  - code, desc\n",
    "\n",
    "\n",
    "4. **calendar** - date dimension\n",
    "  - date, day, week_day, month, month_name, quarter, year, season, season_name\n",
    "\n",
    "\n",
    "5. **ports** - ports of entry codes and cities\n",
    "  - code, city, state\n",
    "\n",
    "\n",
    "6. **airlines** - airlines details\n",
    "  - name, code, country\n",
    "\n",
    "\n",
    "7. **countries** - countries codes and names\n",
    "  - code, country\n",
    "\n",
    "\n",
    "8. **temperatures** - us cities average temperatures\n",
    "  - city, month, avg_temp \n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "*List the steps necessary to pipeline the data into the chosen data model*\n",
    "\n",
    "1. Ingest all raw datasources into pyspark dataframes\n",
    "2. Transform the data using pyspark functions and SQL\n",
    "3. Load the validated and transformed data into the star schema tables, in the datalake trusted zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "*Build the data pipelines to create the data model.*\n",
    "\n",
    "The pipeline was already partially develop in this notebook, more specifically:\n",
    "\n",
    "1. Ingest all raw datasources into pyspark dataframes\n",
    " - was already done in Step 1\n",
    "\n",
    "\n",
    "2. Transform the data using pyspark functions and SQL\n",
    " - was partially done in Step 2, missing only the creation of the last two dimension tables: calendar and i94mode. These two tables are created in the next two steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calendar dim table\n",
    "df = pd.DataFrame({'date': pd.date_range('2000-01-01', '2050-12-31')})\n",
    "df['day'] = df.date.dt.day\n",
    "df['week_day'] = df.date.dt.day_name()\n",
    "df['month'] = df.date.dt.month\n",
    "df['month_name'] = df.date.dt.month_name()\n",
    "df['quarter'] = df.date.dt.quarter\n",
    "df['year'] = df.date.dt.year\n",
    "df['season'] = df.date.dt.month%12 // 3 + 1\n",
    "di = {1: 'winter', 2: 'spring', 3: 'summer', 4: 'fall'}\n",
    "df['season_name'] = df['season'].map(di)\n",
    "\n",
    "cal_schema = StructType([StructField('date', DateType(), True), \\\n",
    "                         StructField('day', IntegerType(), True), \\\n",
    "                         StructField('week_day', StringType(), True), \\\n",
    "                         StructField('month', IntegerType(), True), \\\n",
    "                         StructField('month_name', StringType(), True), \\\n",
    "                         StructField('quarter', IntegerType(), True), \\\n",
    "                         StructField('year', IntegerType(), True), \\\n",
    "                         StructField('season', IntegerType(), True),\n",
    "                         StructField('season_name', StringType(), True)])\n",
    "calendar_df = spark.createDataFrame(df, schema=cal_schema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i94mode dim table\n",
    "columns = ['code','description']\n",
    "data = [(1, 'Air'), (2, 'Sea'), (3, 'Land'), (9, 'Not Reported')]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "i94mode_df = rdd.toDF(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as the last part of our ETL, we need to load all dataframes into parquet files in the datalake trusted zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrivals\n",
    "immig_df.write.parquet('./data/trusted/arrivals/', mode='overwrite')\n",
    "\n",
    "#visa\n",
    "visa_df.write.parquet('./data/trusted/visa/', mode='overwrite')\n",
    "\n",
    "#i94mode\n",
    "i94mode_df.write.parquet('./data/trusted/i94mode/', mode='overwrite')\n",
    "\n",
    "#calendar\n",
    "calendar_df.write.parquet('./data/trusted/calendar/', mode='overwrite')\n",
    "\n",
    "#ports\n",
    "ports_df.write.parquet('./data/trusted/ports/', mode='overwrite')\n",
    "\n",
    "#airlines\n",
    "airlines_df.write.parquet('./data/trusted/airlines/', mode='overwrite')\n",
    "\n",
    "#countries\n",
    "countries_df.write.parquet('./data/trusted/countries/', mode='overwrite')\n",
    "\n",
    "#temperatures\n",
    "temp_df.write.parquet('./data/trusted/temperatures/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "*Explain the data quality checks you'll perform to ensure the pipeline ran as expected.*\n",
    "\n",
    " - Check if the dataframes have data after the transformations were performed.\n",
    " - Check if each table was created successfully as parquet files in its specific directory.\n",
    "\n",
    "*Run Quality Checks:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "tables = [('arrivals', 'immig_df'),\n",
    "          ('visa', 'visa_df'),\n",
    "          ('i94mode', 'i94mode_df'),\n",
    "          ('calendar', 'calendar_df'),\n",
    "          ('ports', 'ports_df'),\n",
    "          ('airlines', 'airlines_df'),\n",
    "          ('countries', 'countries_df'),\n",
    "          ('temperatures', 'temp_df')]\n",
    "\n",
    "\n",
    "# check if the dataframes are not empty\n",
    "dfs_ok = True\n",
    "for table, df in tables:\n",
    "    if eval(df).count() < 1:\n",
    "        dfs_ok = False\n",
    "        print(f'dataframe {df} is empty')\n",
    "\n",
    "if dfs_ok: \n",
    "    print('no empties dataframes')\n",
    "\n",
    "# check if tables as parquet files were successfully generated by the ETL\n",
    "tables_ok = True\n",
    "for table, df in tables:\n",
    "    if not os.path.isfile(f'./data/trusted/{table}/_SUCCESS'):\n",
    "        tables_ok = False\n",
    "        print(f'table {table} was NOT created successfuly')\n",
    "\n",
    "if tables_ok: \n",
    "    print('all tables were created successfully')\n",
    "\n",
    "# check if the amount of rows between dataframes and parquet files match\n",
    "rows_ok = True\n",
    "for table, df in tables:\n",
    "    parquet_count = spark.read.parquet(f'./data/trusted/{table}').count()\n",
    "    df_count = eval(df).count()\n",
    "    if parquet_count != df_count:\n",
    "        rows_ok = False\n",
    "        print(f'parquet files rows count does not match dataframe for table {table}')\n",
    "\n",
    "if rows_ok:\n",
    "    print('all parquet files were created with the same rows count as the respective dataframe')\n",
    "    \n",
    "if dfs_ok and tables_ok and rows_ok: \n",
    "    print('all quality checks passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "*Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.*\n",
    "\n",
    "The data dictionary can be found [here](data-dictionary.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "*Clearly state the rationale for the choice of tools and technologies for the project.*\n",
    "\n",
    "Python was the language of choice since it's broadly used by data engineers and data scientists, with many libraries and the primary language used throughout this course.\n",
    "\n",
    "PySpark was the central library of choice since it enables scalable exploratory analysis using its functions and pure SQL. It also provides excellent performance comparing to other libraries, like pandas, since it runs on top of Apache Spark. Nonetheless, it would also make the application ready to run on an EMR cluster if the amount of data increases or more performance is required.\n",
    "\n",
    "Parquet files in the trusted zone of the datalake are also a good choice since they would easily enable the load to a database, if necessary. Also, if the database of choice is Postgres or Redshift, a single COPY command per table would be enough to transport the data. \n",
    "\n",
    "\n",
    "\n",
    "*Propose how often the data should be updated and why.*\n",
    "\n",
    "The immigration data should be updated weekly, monthly, yearly, or as soon as the US National Tourism and Trade Office releases a new dataset. The same goes for the global temperature data. The other dimension tables don't need to be updated as often since the categorization data doesn't seem to change so frequently.\n",
    "\n",
    "\n",
    "\n",
    "*Write a description of how you would approach the problem differently under the following scenarios:*\n",
    " - *The data was increased by 100x.*\n",
    "\n",
    "In the proposed scenario, I would suggest this ETL script to run on an Apache Hadoop cluster, preferably over Amazon EMR. Also, the datasets could be stored in Amazon S3.\n",
    "\n",
    "\n",
    "\n",
    " - *The data populates a dashboard that must be updated on a daily basis by 7am every day.*\n",
    "\n",
    "I'd recommend using a workflow automation tool, like Apache Airflow, to orchestrate the data pipeline and schedule the daily runs.\n",
    "\n",
    "\n",
    "\n",
    " - *The database needed to be accessed by 100+ people.*\n",
    "\n",
    "I'd use Amazon Redshift as the database of choice. According to the documentation, it supports 500+ simultaneous connections, and it's also a scalable and managed service, providing a reliable tool to host the analytical tables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Write a few analytics queries\n",
    "\n",
    "A notebook with a few analytical queries can be found [here](analytics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
